{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640d5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7279 - loss: 0.7855 - val_accuracy: 0.8467 - val_loss: 0.4284\n",
      "Epoch 2/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8516 - loss: 0.4136 - val_accuracy: 0.8658 - val_loss: 0.3981\n",
      "Epoch 3/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8715 - loss: 0.3575 - val_accuracy: 0.8673 - val_loss: 0.3716\n",
      "Epoch 4/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8772 - loss: 0.3422 - val_accuracy: 0.8728 - val_loss: 0.3466\n",
      "Epoch 5/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8874 - loss: 0.3054 - val_accuracy: 0.8777 - val_loss: 0.3423\n",
      "Epoch 6/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8954 - loss: 0.2858 - val_accuracy: 0.8775 - val_loss: 0.3408\n",
      "Epoch 7/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8985 - loss: 0.2806 - val_accuracy: 0.8803 - val_loss: 0.3307\n",
      "Epoch 8/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8999 - loss: 0.2719 - val_accuracy: 0.8750 - val_loss: 0.3420\n",
      "Epoch 9/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9056 - loss: 0.2508 - val_accuracy: 0.8803 - val_loss: 0.3364\n",
      "Epoch 10/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9053 - loss: 0.2521 - val_accuracy: 0.8837 - val_loss: 0.3310\n",
      "Epoch 11/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9097 - loss: 0.2428 - val_accuracy: 0.8860 - val_loss: 0.3243\n",
      "Epoch 12/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9147 - loss: 0.2308 - val_accuracy: 0.8868 - val_loss: 0.3190\n",
      "Epoch 13/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9157 - loss: 0.2257 - val_accuracy: 0.8850 - val_loss: 0.3302\n",
      "Epoch 14/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9184 - loss: 0.2159 - val_accuracy: 0.8858 - val_loss: 0.3239\n",
      "Epoch 15/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9208 - loss: 0.2111 - val_accuracy: 0.8900 - val_loss: 0.3265\n",
      "Epoch 16/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9255 - loss: 0.2002 - val_accuracy: 0.8875 - val_loss: 0.3185\n",
      "Epoch 17/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9252 - loss: 0.1996 - val_accuracy: 0.8850 - val_loss: 0.3331\n",
      "Epoch 18/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9274 - loss: 0.1938 - val_accuracy: 0.8862 - val_loss: 0.3412\n",
      "Epoch 19/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9314 - loss: 0.1827 - val_accuracy: 0.8903 - val_loss: 0.3189\n",
      "Epoch 20/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9306 - loss: 0.1830 - val_accuracy: 0.8885 - val_loss: 0.3316\n",
      "Epoch 21/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9328 - loss: 0.1766 - val_accuracy: 0.8875 - val_loss: 0.3501\n",
      "Epoch 22/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9347 - loss: 0.1737 - val_accuracy: 0.8867 - val_loss: 0.3421\n",
      "Epoch 23/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9360 - loss: 0.1721 - val_accuracy: 0.8903 - val_loss: 0.3492\n",
      "Epoch 24/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9393 - loss: 0.1640 - val_accuracy: 0.8860 - val_loss: 0.3569\n",
      "Epoch 25/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9396 - loss: 0.1595 - val_accuracy: 0.8940 - val_loss: 0.3411\n",
      "Epoch 26/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9404 - loss: 0.1603 - val_accuracy: 0.8863 - val_loss: 0.3671\n",
      "Epoch 27/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9455 - loss: 0.1470 - val_accuracy: 0.8877 - val_loss: 0.3562\n",
      "Epoch 28/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9455 - loss: 0.1478 - val_accuracy: 0.8890 - val_loss: 0.3772\n",
      "Epoch 29/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9470 - loss: 0.1435 - val_accuracy: 0.8933 - val_loss: 0.3642\n",
      "Epoch 30/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9490 - loss: 0.1349 - val_accuracy: 0.8895 - val_loss: 0.3785\n",
      "Epoch 31/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9476 - loss: 0.1383 - val_accuracy: 0.8870 - val_loss: 0.3804\n",
      "Epoch 32/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9492 - loss: 0.1334 - val_accuracy: 0.8922 - val_loss: 0.3816\n",
      "Epoch 33/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9504 - loss: 0.1275 - val_accuracy: 0.8878 - val_loss: 0.4065\n",
      "Epoch 34/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9515 - loss: 0.1275 - val_accuracy: 0.8923 - val_loss: 0.3908\n",
      "Epoch 35/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9548 - loss: 0.1211 - val_accuracy: 0.8870 - val_loss: 0.4007\n",
      "Epoch 36/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9533 - loss: 0.1231 - val_accuracy: 0.8898 - val_loss: 0.3923\n",
      "Epoch 37/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9572 - loss: 0.1177 - val_accuracy: 0.8853 - val_loss: 0.4294\n",
      "Epoch 38/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9538 - loss: 0.1240 - val_accuracy: 0.8898 - val_loss: 0.4114\n",
      "Epoch 39/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9584 - loss: 0.1115 - val_accuracy: 0.8875 - val_loss: 0.4192\n",
      "Epoch 40/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9597 - loss: 0.1089 - val_accuracy: 0.8885 - val_loss: 0.4248\n",
      "Epoch 41/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9596 - loss: 0.1067 - val_accuracy: 0.8807 - val_loss: 0.4457\n",
      "Epoch 42/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9605 - loss: 0.1037 - val_accuracy: 0.8907 - val_loss: 0.4332\n",
      "Epoch 43/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9611 - loss: 0.1049 - val_accuracy: 0.8925 - val_loss: 0.4388\n",
      "Epoch 44/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9648 - loss: 0.0963 - val_accuracy: 0.8917 - val_loss: 0.4273\n",
      "Epoch 45/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9625 - loss: 0.0992 - val_accuracy: 0.8915 - val_loss: 0.4617\n",
      "Epoch 46/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9663 - loss: 0.0931 - val_accuracy: 0.8893 - val_loss: 0.4756\n",
      "Epoch 47/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9653 - loss: 0.0915 - val_accuracy: 0.8878 - val_loss: 0.4472\n",
      "Epoch 48/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9671 - loss: 0.0876 - val_accuracy: 0.8908 - val_loss: 0.4637\n",
      "Epoch 49/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9650 - loss: 0.0935 - val_accuracy: 0.8873 - val_loss: 0.4866\n",
      "Epoch 50/50\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9655 - loss: 0.0877 - val_accuracy: 0.8912 - val_loss: 0.4751\n",
      "模型架構已儲存為 fashion_mnist.json，權重已儲存為 fashion_mnist.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# === Activation functions ===\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "# === Flatten ===\n",
    "def flatten(x):\n",
    "    return x.reshape(x.shape[0], -1)\n",
    "\n",
    "# === Dense layer ===\n",
    "def dense(x, W, b):\n",
    "    return x @ W + b\n",
    "\n",
    "# Infer TensorFlow h5 model using numpy\n",
    "# Support only Dense, Flatten, relu, softmax now\n",
    "def nn_forward_h5(model_arch, weights, data):\n",
    "    x = data\n",
    "    for layer in model_arch:\n",
    "        lname = layer['name']\n",
    "        ltype = layer['type']\n",
    "        cfg = layer['config']\n",
    "        wnames = layer['weights']\n",
    "\n",
    "        if ltype == \"Flatten\":\n",
    "            x = flatten(x)\n",
    "        elif ltype == \"Dense\":\n",
    "            W = weights[wnames[0]]\n",
    "            b = weights[wnames[1]]\n",
    "            x = dense(x, W, b)\n",
    "            if cfg.get(\"activation\") == \"relu\":\n",
    "                x = relu(x)\n",
    "            elif cfg.get(\"activation\") == \"softmax\":\n",
    "                x = softmax(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# You are free to replace nn_forward_h5() with your own implementation \n",
    "def nn_inference(model_arch, weights, data):\n",
    "    return nn_forward_h5(model_arch, weights, data)\n",
    "\n",
    "# === 以下為訓練與儲存模型架構/權重 ===\n",
    "def train_and_save():\n",
    "\n",
    "\n",
    "    # 1. 載入資料\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    x_train = x_train.astype(np.float32) / 255.0\n",
    "    x_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "    # 2. 建立模型\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28), name=\"flatten\"),\n",
    "        tf.keras.layers.Dense(128, activation='relu', name=\"dense1\"),\n",
    "        tf.keras.layers.Dense(64, activation='relu', name=\"dense2\"),\n",
    "        tf.keras.layers.Dense(10, activation='softmax', name=\"dense3\")\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # 3. 訓練模型\n",
    "    model.fit(x_train, y_train, epochs=50, batch_size=128, validation_split=0.1)\n",
    "\n",
    "    # 4. 儲存模型架構（簡化為自定格式，方便 numpy 推論）\n",
    "    model_arch = []\n",
    "    for layer in model.layers:\n",
    "        ltype = type(layer).__name__\n",
    "        lname = layer.name\n",
    "        cfg = {}\n",
    "        if ltype == \"Dense\":\n",
    "            cfg[\"activation\"] = layer.activation.__name__\n",
    "            cfg[\"units\"] = layer.units\n",
    "        elif ltype == \"Flatten\":\n",
    "            pass\n",
    "        # 權重名稱\n",
    "        wnames = []\n",
    "        for i, w in enumerate(layer.get_weights()):\n",
    "            wnames.append(f\"{lname}_w{i}\")\n",
    "        model_arch.append({\n",
    "            \"name\": lname,\n",
    "            \"type\": ltype,\n",
    "            \"config\": cfg,\n",
    "            \"weights\": wnames\n",
    "        })\n",
    "    with open(\"fashion_mnist.json\", \"w\") as f:\n",
    "        json.dump(model_arch, f)\n",
    "\n",
    "    # 5. 儲存權重為 npz\n",
    "    weights = {}\n",
    "    for layer in model.layers:\n",
    "        for idx, w in enumerate(layer.get_weights()):\n",
    "            weights[f\"{layer.name}_w{idx}\"] = w\n",
    "    np.savez(\"fashion_mnist.npz\", **weights)\n",
    "\n",
    "    print(\"模型架構已儲存為 fashion_mnist.json，權重已儲存為 fashion_mnist.npz\")\n",
    "\n",
    "# === 主程式 ===\n",
    "if __name__ == \"__main__\":\n",
    "        train_and_save()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
